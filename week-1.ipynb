{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18193ed2-4bf3-4139-ba44-d38492a056fd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Geospatial Data Engineering Associate\n",
    "\n",
    "**What you will learn**\n",
    "This notebook will teach you to:\n",
    "\n",
    "* Set up your environment and create a **WherobotsDB context**\n",
    "* Load vector and raster datasets directly from **AWS S3** into **Apache Sedona DataFrames**\n",
    "* Inspect and validate geometries for quality and consistency\n",
    "* Standardize and transform **Coordinate Reference Systems (CRS)**\n",
    "* Apply the **Bronze → Silver → Gold** data architecture pattern for spatial workflows\n",
    "* Save and manage your first **Iceberg table** to prepare for scalable analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3100f77-fe3f-4c9a-8a0a-a4d62c723e50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T17:39:33.562606Z",
     "iopub.status.busy": "2026-02-02T17:39:33.562451Z",
     "iopub.status.idle": "2026-02-02T17:39:34.661382Z",
     "shell.execute_reply": "2026-02-02T17:39:34.660482Z",
     "shell.execute_reply.started": "2026-02-02T17:39:33.562588Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1105/2854814182.py:3: DeprecationWarning: Importing from 'sedona.sql' is deprecated. Please use 'sedona.spark.sql' instead.\n",
      "  from sedona.sql.st_functions import ST_IsValid, ST_IsValidReason, ST_MakeValid\n",
      "/tmp/ipykernel_1105/2854814182.py:3: DeprecationWarning: Importing from 'sedona.sql.st_functions' is deprecated. Please use 'sedona.spark.sql.st_functions' instead.\n",
      "  from sedona.sql.st_functions import ST_IsValid, ST_IsValidReason, ST_MakeValid\n"
     ]
    }
   ],
   "source": [
    "from sedona.spark import *\n",
    "from pyspark.sql.functions import col, when, expr\n",
    "from sedona.sql.st_functions import ST_IsValid, ST_IsValidReason, ST_MakeValid\n",
    "from pyspark.sql import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8625818a-f65d-44c6-a5f2-a7d149c5075b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T17:39:34.667182Z",
     "iopub.status.busy": "2026-02-02T17:39:34.667080Z",
     "iopub.status.idle": "2026-02-02T17:41:17.030550Z",
     "shell.execute_reply": "2026-02-02T17:41:17.029488Z",
     "shell.execute_reply.started": "2026-02-02T17:39:34.667168Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "Setting Spark log level to \"WARN\".\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "config = SedonaContext.builder().getOrCreate()\n",
    "sedona = SedonaContext.create(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78e0763",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Intoduction to a Sedona DataFrame\n",
    "\n",
    "A Sedona DataFrame is an extension of the standard Spark DataFrame that adds native support for geospatial data types — namely Vector and Raster.\n",
    "\n",
    "- Vector data includes geometries such as points, lines, and polygons.\n",
    "- Raster represents gridded or image-based data such as satellite imagery or elevation tiles.\n",
    "\n",
    "Because these are built-in (native) data types, Sedona and Wherobots know how to handle them automatically.\n",
    "This means you can:\n",
    "- Run spatial and raster functions (like `ST_Contains`, `ST_Intersection`, `RS_Clip`, `RS_ZonalStats`) directly in your queries.\n",
    "- Combine vector and raster data in the same workflow — for example, clipping imagery to a city boundary.\n",
    "- Scale these operations easily across large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95681fa0",
   "metadata": {},
   "source": [
    "## Loading vector data into a Sedona DataFrame\n",
    "\n",
    "Now that we know what a Sedona DataFrame is, let’s see how to load vector data into one.\n",
    "\n",
    "In this section, we’ll cover:\n",
    "- GeoParquet — the most common cloud-native geospatial format\n",
    "- GeoJSON — flexible and human-readable\n",
    "- CSV — raw text with WKT/WKB geometries\n",
    "- Shapefile — the classic desktop GIS format\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591cdee0",
   "metadata": {},
   "source": [
    "### GeoParquet\n",
    "\n",
    "GeoParquet is the preferred format for storing and sharing vector data in the cloud.\n",
    "\n",
    "It extends the standard Apache Parquet format with a small block of “geo” metadata that describes:\n",
    "- which column contains the geometry,\n",
    "- the geometry type (Point, Polygon, etc.),\n",
    "- its coordinate reference system (CRS), and\n",
    "- the bounding box of each geometry column.\n",
    "\n",
    "Because GeoParquet is columnar, compressed, and splittable, it’s ideal for large-scale analytics on cloud object storage like S3.\n",
    "It also supports spatial predicate push-down — meaning Wherobots can automatically skip reading irrelevant files and row-groups when performing spatial range queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1210df95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T17:41:17.032999Z",
     "iopub.status.busy": "2026-02-02T17:41:17.032841Z",
     "iopub.status.idle": "2026-02-02T17:41:18.045643Z",
     "shell.execute_reply": "2026-02-02T17:41:18.044540Z",
     "shell.execute_reply.started": "2026-02-02T17:41:17.032978Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/02 17:41:17 WARN WherobotsStIntCredentialsProvider: Your S3 storage integration with the IAM Role arn:aws:iam::306385193173:role/wherobots-examples-storage-integration-role depends on a deprecating trust relationship policy.\n",
      "We highly recommend migrating this as soon as possible to avoid any possible service disruptions.\n",
      "Please visit https://cloud.wherobots.com/migrate-storage for migration guidance.\n",
      "\n",
      "26/02/02 17:41:17 WARN FileSystem: Failed to initialize filesystem s3://wherobots-examples/data/nyc_buildings.parquet: java.nio.file.AccessDeniedException: : Instantiate com.wherobots.fs.s3a.WherobotsStIntCredentialsProvider: com.amazonaws.services.securitytoken.model.AWSSecurityTokenServiceException: User: arn:aws:sts::329898491045:assumed-role/wherobots-user-principals/wbc-jf3gkm4ile-besg0oop07pktb is not authorized to perform: sts:AssumeRole on resource: arn:aws:iam::306385193173:role/wherobots-examples-storage-integration-role (Service: AWSSecurityTokenService; Status Code: 403; Error Code: AccessDenied; Request ID: c8a7d5a0-cfc8-4ad9-9ecd-00093c38f81b; Proxy: null):AccessDenied\n",
      "26/02/02 17:41:17 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: s3://wherobots-examples/data/nyc_buildings.parquet.\n",
      "java.nio.file.AccessDeniedException: : Instantiate com.wherobots.fs.s3a.WherobotsStIntCredentialsProvider: com.amazonaws.services.securitytoken.model.AWSSecurityTokenServiceException: User: arn:aws:sts::329898491045:assumed-role/wherobots-user-principals/wbc-jf3gkm4ile-besg0oop07pktb is not authorized to perform: sts:AssumeRole on resource: arn:aws:iam::306385193173:role/wherobots-examples-storage-integration-role (Service: AWSSecurityTokenService; Status Code: 403; Error Code: AccessDenied; Request ID: c8a7d5a0-cfc8-4ad9-9ecd-00093c38f81b; Proxy: null):AccessDenied\n",
      "\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:253)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AUtils.createAWSCredentialProvider(S3AUtils.java:745)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AUtils.buildAWSProviderList(S3AUtils.java:659)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AUtils.createAWSCredentialProviderSet(S3AUtils.java:585)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.bindAWSClient(S3AFileSystem.java:959)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:586)\n",
      "\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3611)\n",
      "\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3712)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3663)\n",
      "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)\n",
      "\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:366)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:186)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: com.amazonaws.services.securitytoken.model.AWSSecurityTokenServiceException: User: arn:aws:sts::329898491045:assumed-role/wherobots-user-principals/wbc-jf3gkm4ile-besg0oop07pktb is not authorized to perform: sts:AssumeRole on resource: arn:aws:iam::306385193173:role/wherobots-examples-storage-integration-role (Service: AWSSecurityTokenService; Status Code: 403; Error Code: AccessDenied; Request ID: c8a7d5a0-cfc8-4ad9-9ecd-00093c38f81b; Proxy: null)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1880)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1418)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1387)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1157)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)\n",
      "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)\n",
      "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)\n",
      "\tat com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClient.doInvoke(AWSSecurityTokenServiceClient.java:1731)\n",
      "\tat com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClient.invoke(AWSSecurityTokenServiceClient.java:1698)\n",
      "\tat com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClient.invoke(AWSSecurityTokenServiceClient.java:1687)\n",
      "\tat com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClient.executeAssumeRole(AWSSecurityTokenServiceClient.java:532)\n",
      "\tat com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClient.assumeRole(AWSSecurityTokenServiceClient.java:501)\n",
      "\tat com.amazonaws.auth.STSAssumeRoleSessionCredentialsProvider.newSession(STSAssumeRoleSessionCredentialsProvider.java:360)\n",
      "\tat com.amazonaws.auth.STSAssumeRoleSessionCredentialsProvider.access$000(STSAssumeRoleSessionCredentialsProvider.java:44)\n",
      "\tat com.amazonaws.auth.STSAssumeRoleSessionCredentialsProvider$1.call(STSAssumeRoleSessionCredentialsProvider.java:101)\n",
      "\tat com.amazonaws.auth.STSAssumeRoleSessionCredentialsProvider$1.call(STSAssumeRoleSessionCredentialsProvider.java:98)\n",
      "\tat com.amazonaws.auth.RefreshableTask.refreshValue(RefreshableTask.java:295)\n",
      "\tat com.amazonaws.auth.RefreshableTask.blockingRefresh(RefreshableTask.java:251)\n",
      "\tat com.amazonaws.auth.RefreshableTask.getValue(RefreshableTask.java:192)\n",
      "\tat com.amazonaws.auth.STSAssumeRoleSessionCredentialsProvider.getCredentials(STSAssumeRoleSessionCredentialsProvider.java:329)\n",
      "\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:468)\n",
      "\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:431)\n",
      "\tat com.wherobots.fs.s3a.WherobotsStIntCredentialsProvider.getCredentials(WherobotsStIntCredentialsProvider.java:184)\n",
      "\tat com.wherobots.fs.s3a.WherobotsStIntCredentialsProvider.<init>(WherobotsStIntCredentialsProvider.java:163)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AUtils.createAWSCredentialProvider(S3AUtils.java:706)\n",
      "\t... 29 more\n",
      "26/02/02 17:41:17 WARN WherobotsStIntCredentialsProvider: Your S3 storage integration with the IAM Role arn:aws:iam::306385193173:role/wherobots-examples-storage-integration-role depends on a deprecating trust relationship policy.\n",
      "We highly recommend migrating this as soon as possible to avoid any possible service disruptions.\n",
      "Please visit https://cloud.wherobots.com/migrate-storage for migration guidance.\n",
      "\n",
      "26/02/02 17:41:17 WARN FileSystem: Failed to initialize filesystem s3://wherobots-examples/data/nyc_buildings.parquet: java.nio.file.AccessDeniedException: : Instantiate com.wherobots.fs.s3a.WherobotsStIntCredentialsProvider: com.amazonaws.services.securitytoken.model.AWSSecurityTokenServiceException: User: arn:aws:sts::329898491045:assumed-role/wherobots-user-principals/wbc-jf3gkm4ile-besg0oop07pktb is not authorized to perform: sts:AssumeRole on resource: arn:aws:iam::306385193173:role/wherobots-examples-storage-integration-role (Service: AWSSecurityTokenService; Status Code: 403; Error Code: AccessDenied; Request ID: 0b75af6b-eb32-4c0d-bfd5-f43f0b3b3b76; Proxy: null):AccessDenied\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o264.load.\n: java.nio.file.AccessDeniedException: : Instantiate com.wherobots.fs.s3a.WherobotsStIntCredentialsProvider: com.amazonaws.services.securitytoken.model.AWSSecurityTokenServiceException: User: arn:aws:sts::329898491045:assumed-role/wherobots-user-principals/wbc-jf3gkm4ile-besg0oop07pktb is not authorized to perform: sts:AssumeRole on resource: arn:aws:iam::306385193173:role/wherobots-examples-storage-integration-role (Service: AWSSecurityTokenService; Status Code: 403; Error Code: AccessDenied; Request ID: 0b75af6b-eb32-4c0d-bfd5-f43f0b3b3b76; Proxy: null):AccessDenied\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:253)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.createAWSCredentialProvider(S3AUtils.java:745)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.buildAWSProviderList(S3AUtils.java:659)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.createAWSCredentialProviderSet(S3AUtils.java:585)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.bindAWSClient(S3AFileSystem.java:959)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:586)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3611)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3712)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3663)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:724)\n\tat scala.collection.immutable.List.map(List.scala:246)\n\tat scala.collection.immutable.List.map(List.scala:79)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:722)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:551)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:404)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:186)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: com.amazonaws.services.securitytoken.model.AWSSecurityTokenServiceException: User: arn:aws:sts::329898491045:assumed-role/wherobots-user-principals/wbc-jf3gkm4ile-besg0oop07pktb is not authorized to perform: sts:AssumeRole on resource: arn:aws:iam::306385193173:role/wherobots-examples-storage-integration-role (Service: AWSSecurityTokenService; Status Code: 403; Error Code: AccessDenied; Request ID: 0b75af6b-eb32-4c0d-bfd5-f43f0b3b3b76; Proxy: null)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1880)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1418)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1387)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1157)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)\n\tat com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClient.doInvoke(AWSSecurityTokenServiceClient.java:1731)\n\tat com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClient.invoke(AWSSecurityTokenServiceClient.java:1698)\n\tat com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClient.invoke(AWSSecurityTokenServiceClient.java:1687)\n\tat com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClient.executeAssumeRole(AWSSecurityTokenServiceClient.java:532)\n\tat com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClient.assumeRole(AWSSecurityTokenServiceClient.java:501)\n\tat com.amazonaws.auth.STSAssumeRoleSessionCredentialsProvider.newSession(STSAssumeRoleSessionCredentialsProvider.java:360)\n\tat com.amazonaws.auth.STSAssumeRoleSessionCredentialsProvider.access$000(STSAssumeRoleSessionCredentialsProvider.java:44)\n\tat com.amazonaws.auth.STSAssumeRoleSessionCredentialsProvider$1.call(STSAssumeRoleSessionCredentialsProvider.java:101)\n\tat com.amazonaws.auth.STSAssumeRoleSessionCredentialsProvider$1.call(STSAssumeRoleSessionCredentialsProvider.java:98)\n\tat com.amazonaws.auth.RefreshableTask.refreshValue(RefreshableTask.java:295)\n\tat com.amazonaws.auth.RefreshableTask.blockingRefresh(RefreshableTask.java:251)\n\tat com.amazonaws.auth.RefreshableTask.getValue(RefreshableTask.java:192)\n\tat com.amazonaws.auth.STSAssumeRoleSessionCredentialsProvider.getCredentials(STSAssumeRoleSessionCredentialsProvider.java:329)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:468)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:431)\n\tat com.wherobots.fs.s3a.WherobotsStIntCredentialsProvider.getCredentials(WherobotsStIntCredentialsProvider.java:184)\n\tat com.wherobots.fs.s3a.WherobotsStIntCredentialsProvider.<init>(WherobotsStIntCredentialsProvider.java:163)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.createAWSCredentialProvider(S3AUtils.java:706)\n\t... 33 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m geoparquet_path = \u001b[33m\"\u001b[39m\u001b[33ms3://wherobots-examples/data/nyc_buildings.parquet\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m df = \u001b[43msedona\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgeoparquet\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeoparquet_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m df.show(\u001b[32m5\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/spark/python/pyspark/sql/readwriter.py:307\u001b[39m, in \u001b[36mDataFrameReader.load\u001b[39m\u001b[34m(self, path, format, schema, **options)\u001b[39m\n\u001b[32m    305\u001b[39m \u001b[38;5;28mself\u001b[39m.options(**options)\n\u001b[32m    306\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jreader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    308\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    309\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) != \u001b[38;5;28mlist\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/wherobots/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdeco\u001b[39m(*a: Any, **kw: Any) -> Any:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m         converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/wherobots/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    324\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o264.load.\n: java.nio.file.AccessDeniedException: : Instantiate com.wherobots.fs.s3a.WherobotsStIntCredentialsProvider: com.amazonaws.services.securitytoken.model.AWSSecurityTokenServiceException: User: arn:aws:sts::329898491045:assumed-role/wherobots-user-principals/wbc-jf3gkm4ile-besg0oop07pktb is not authorized to perform: sts:AssumeRole on resource: arn:aws:iam::306385193173:role/wherobots-examples-storage-integration-role (Service: AWSSecurityTokenService; Status Code: 403; Error Code: AccessDenied; Request ID: 0b75af6b-eb32-4c0d-bfd5-f43f0b3b3b76; Proxy: null):AccessDenied\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:253)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.createAWSCredentialProvider(S3AUtils.java:745)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.buildAWSProviderList(S3AUtils.java:659)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.createAWSCredentialProviderSet(S3AUtils.java:585)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.bindAWSClient(S3AFileSystem.java:959)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:586)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3611)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3712)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3663)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:724)\n\tat scala.collection.immutable.List.map(List.scala:246)\n\tat scala.collection.immutable.List.map(List.scala:79)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:722)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:551)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:404)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:186)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: com.amazonaws.services.securitytoken.model.AWSSecurityTokenServiceException: User: arn:aws:sts::329898491045:assumed-role/wherobots-user-principals/wbc-jf3gkm4ile-besg0oop07pktb is not authorized to perform: sts:AssumeRole on resource: arn:aws:iam::306385193173:role/wherobots-examples-storage-integration-role (Service: AWSSecurityTokenService; Status Code: 403; Error Code: AccessDenied; Request ID: 0b75af6b-eb32-4c0d-bfd5-f43f0b3b3b76; Proxy: null)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1880)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1418)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1387)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1157)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)\n\tat com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClient.doInvoke(AWSSecurityTokenServiceClient.java:1731)\n\tat com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClient.invoke(AWSSecurityTokenServiceClient.java:1698)\n\tat com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClient.invoke(AWSSecurityTokenServiceClient.java:1687)\n\tat com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClient.executeAssumeRole(AWSSecurityTokenServiceClient.java:532)\n\tat com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClient.assumeRole(AWSSecurityTokenServiceClient.java:501)\n\tat com.amazonaws.auth.STSAssumeRoleSessionCredentialsProvider.newSession(STSAssumeRoleSessionCredentialsProvider.java:360)\n\tat com.amazonaws.auth.STSAssumeRoleSessionCredentialsProvider.access$000(STSAssumeRoleSessionCredentialsProvider.java:44)\n\tat com.amazonaws.auth.STSAssumeRoleSessionCredentialsProvider$1.call(STSAssumeRoleSessionCredentialsProvider.java:101)\n\tat com.amazonaws.auth.STSAssumeRoleSessionCredentialsProvider$1.call(STSAssumeRoleSessionCredentialsProvider.java:98)\n\tat com.amazonaws.auth.RefreshableTask.refreshValue(RefreshableTask.java:295)\n\tat com.amazonaws.auth.RefreshableTask.blockingRefresh(RefreshableTask.java:251)\n\tat com.amazonaws.auth.RefreshableTask.getValue(RefreshableTask.java:192)\n\tat com.amazonaws.auth.STSAssumeRoleSessionCredentialsProvider.getCredentials(STSAssumeRoleSessionCredentialsProvider.java:329)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:468)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:431)\n\tat com.wherobots.fs.s3a.WherobotsStIntCredentialsProvider.getCredentials(WherobotsStIntCredentialsProvider.java:184)\n\tat com.wherobots.fs.s3a.WherobotsStIntCredentialsProvider.<init>(WherobotsStIntCredentialsProvider.java:163)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.createAWSCredentialProvider(S3AUtils.java:706)\n\t... 33 more\n"
     ]
    }
   ],
   "source": [
    "geoparquet_path = \"s3://wherobots-examples/data/nyc_buildings.parquet\"\n",
    "\n",
    "df = sedona.read.format(\"geoparquet\").load(geoparquet_path)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3851be38",
   "metadata": {},
   "source": [
    "### GeoJSON\n",
    "\n",
    "Wherobots supports reading GeoJSON files directly using the geojson data source.\n",
    "\n",
    "This reader understands most GeoJSON variations, including:\n",
    "- Standard Feature and FeatureCollection objects\n",
    "- SpatioTemporal Asset Catalog (STAC) files\n",
    "- GeoJSON files that span multiple lines for readability\n",
    "\n",
    "When loaded, Wherobots automatically parses the geometry field into its internal Geometry type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8039e5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T17:41:55.982102Z",
     "iopub.status.busy": "2026-02-02T17:41:55.981913Z",
     "iopub.status.idle": "2026-02-02T17:41:56.603377Z",
     "shell.execute_reply": "2026-02-02T17:41:56.602596Z",
     "shell.execute_reply.started": "2026-02-02T17:41:55.982086Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/02 17:41:56 WARN WherobotsStIntCredentialsProvider: Your S3 storage integration with the IAM Role arn:aws:iam::306385193173:role/wherobots-examples-storage-integration-role depends on a deprecating trust relationship policy.\n",
      "We highly recommend migrating this as soon as possible to avoid any possible service disruptions.\n",
      "Please visit https://cloud.wherobots.com/migrate-storage for migration guidance.\n",
      "\n",
      "26/02/02 17:41:56 WARN FileSystem: Failed to initialize filesystem s3://wherobots-examples/data/noaa/storms: java.nio.file.AccessDeniedException: : Instantiate com.wherobots.fs.s3a.WherobotsStIntCredentialsProvider: com.amazonaws.services.securitytoken.model.AWSSecurityTokenServiceException: User: arn:aws:sts::329898491045:assumed-role/wherobots-user-principals/wbc-jf3gkm4ile-besg0oop07pktb is not authorized to perform: sts:AssumeRole on resource: arn:aws:iam::306385193173:role/wherobots-examples-storage-integration-role (Service: AWSSecurityTokenService; Status Code: 403; Error Code: AccessDenied; Request ID: 9b16ee24-c576-4147-a5ac-77d4d31d6a80; Proxy: null):AccessDenied\n",
      "26/02/02 17:41:56 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: s3://wherobots-examples/data/noaa/storms/.\n",
      "java.nio.file.AccessDeniedException: : Instantiate com.wherobots.fs.s3a.WherobotsStIntCredentialsProvider: com.amazonaws.services.securitytoken.model.AWSSecurityTokenServiceException: User: arn:aws:sts::329898491045:assumed-role/wherobots-user-principals/wbc-jf3gkm4ile-besg0oop07pktb is not authorized to perform: sts:AssumeRole on resource: arn:aws:iam::306385193173:role/wherobots-examples-storage-integration-role (Service: AWSSecurityTokenService; Status Code: 403; Error Code: AccessDenied; Request ID: 9b16ee24-c576-4147-a5ac-77d4d31d6a80; Proxy: null):AccessDenied\n",
      "\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:253)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AUtils.createAWSCredentialProvider(S3AUtils.java:745)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AUtils.buildAWSProviderList(S3AUtils.java:659)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AUtils.createAWSCredentialProviderSet(S3AUtils.java:585)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.bindAWSClient(S3AFileSystem.java:959)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:586)\n",
      "\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3611)\n",
      "\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3712)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3663)\n",
      "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)\n",
      "\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:366)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:186)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: com.amazonaws.services.securitytoken.model.AWSSecurityTokenServiceException: User: arn:aws:sts::329898491045:assumed-role/wherobots-user-principals/wbc-jf3gkm4ile-besg0oop07pktb is not authorized to perform: sts:AssumeRole on resource: arn:aws:iam::306385193173:role/wherobots-examples-storage-integration-role (Service: AWSSecurityTokenService; Status Code: 403; Error Code: AccessDenied; Request ID: 9b16ee24-c576-4147-a5ac-77d4d31d6a80; Proxy: null)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1880)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1418)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1387)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1157)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)\n",
      "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)\n",
      "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)\n",
      "\tat com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClient.doInvoke(AWSSecurityTokenServiceClient.java:1731)\n",
      "\tat com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClient.invoke(AWSSecurityTokenServiceClient.java:1698)\n",
      "\tat com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClient.invoke(AWSSecurityTokenServiceClient.java:1687)\n",
      "\tat com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClient.executeAssumeRole(AWSSecurityTokenServiceClient.java:532)\n",
      "\tat com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClient.assumeRole(AWSSecurityTokenServiceClient.java:501)\n",
      "\tat com.amazonaws.auth.STSAssumeRoleSessionCredentialsProvider.newSession(STSAssumeRoleSessionCredentialsProvider.java:360)\n",
      "\tat com.amazonaws.auth.STSAssumeRoleSessionCredentialsProvider.access$000(STSAssumeRoleSessionCredentialsProvider.java:44)\n",
      "\tat com.amazonaws.auth.STSAssumeRoleSessionCredentialsProvider$1.call(STSAssumeRoleSessionCredentialsProvider.java:101)\n",
      "\tat com.amazonaws.auth.STSAssumeRoleSessionCredentialsProvider$1.call(STSAssumeRoleSessionCredentialsProvider.java:98)\n",
      "\tat com.amazonaws.auth.RefreshableTask.refreshValue(RefreshableTask.java:295)\n",
      "\tat com.amazonaws.auth.RefreshableTask.blockingRefresh(RefreshableTask.java:251)\n",
      "\tat com.amazonaws.auth.RefreshableTask.getValue(RefreshableTask.java:192)\n",
      "\tat com.amazonaws.auth.STSAssumeRoleSessionCredentialsProvider.getCredentials(STSAssumeRoleSessionCredentialsProvider.java:329)\n",
      "\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:468)\n",
      "\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:431)\n",
      "\tat com.wherobots.fs.s3a.WherobotsStIntCredentialsProvider.getCredentials(WherobotsStIntCredentialsProvider.java:184)\n",
      "\tat com.wherobots.fs.s3a.WherobotsStIntCredentialsProvider.<init>(WherobotsStIntCredentialsProvider.java:163)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AUtils.createAWSCredentialProvider(S3AUtils.java:706)\n",
      "\t... 29 more\n",
      "26/02/02 17:41:56 WARN WherobotsStIntCredentialsProvider: Your S3 storage integration with the IAM Role arn:aws:iam::306385193173:role/wherobots-examples-storage-integration-role depends on a deprecating trust relationship policy.\n",
      "We highly recommend migrating this as soon as possible to avoid any possible service disruptions.\n",
      "Please visit https://cloud.wherobots.com/migrate-storage for migration guidance.\n",
      "\n",
      "26/02/02 17:41:56 WARN FileSystem: Failed to initialize filesystem s3://wherobots-examples/data/noaa/storms: java.nio.file.AccessDeniedException: : Instantiate com.wherobots.fs.s3a.WherobotsStIntCredentialsProvider: com.amazonaws.services.securitytoken.model.AWSSecurityTokenServiceException: User: arn:aws:sts::329898491045:assumed-role/wherobots-user-principals/wbc-jf3gkm4ile-besg0oop07pktb is not authorized to perform: sts:AssumeRole on resource: arn:aws:iam::306385193173:role/wherobots-examples-storage-integration-role (Service: AWSSecurityTokenService; Status Code: 403; Error Code: AccessDenied; Request ID: 7fb17b56-88ea-4046-a6ed-4a4c5c382e37; Proxy: null):AccessDenied\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o269.load.\n: java.nio.file.AccessDeniedException: : Instantiate com.wherobots.fs.s3a.WherobotsStIntCredentialsProvider: com.amazonaws.services.securitytoken.model.AWSSecurityTokenServiceException: User: arn:aws:sts::329898491045:assumed-role/wherobots-user-principals/wbc-jf3gkm4ile-besg0oop07pktb is not authorized to perform: sts:AssumeRole on resource: arn:aws:iam::306385193173:role/wherobots-examples-storage-integration-role (Service: AWSSecurityTokenService; Status Code: 403; Error Code: AccessDenied; Request ID: 7fb17b56-88ea-4046-a6ed-4a4c5c382e37; Proxy: null):AccessDenied\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:253)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.createAWSCredentialProvider(S3AUtils.java:745)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.buildAWSProviderList(S3AUtils.java:659)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.createAWSCredentialProviderSet(S3AUtils.java:585)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.bindAWSClient(S3AFileSystem.java:959)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:586)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3611)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3712)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3663)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:724)\n\tat scala.collection.immutable.List.map(List.scala:246)\n\tat scala.collection.immutable.List.map(List.scala:79)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:722)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:551)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:404)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:186)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: com.amazonaws.services.securitytoken.model.AWSSecurityTokenServiceException: User: arn:aws:sts::329898491045:assumed-role/wherobots-user-principals/wbc-jf3gkm4ile-besg0oop07pktb is not authorized to perform: sts:AssumeRole on resource: arn:aws:iam::306385193173:role/wherobots-examples-storage-integration-role (Service: AWSSecurityTokenService; Status Code: 403; Error Code: AccessDenied; Request ID: 7fb17b56-88ea-4046-a6ed-4a4c5c382e37; Proxy: null)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1880)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1418)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1387)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1157)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)\n\tat com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClient.doInvoke(AWSSecurityTokenServiceClient.java:1731)\n\tat com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClient.invoke(AWSSecurityTokenServiceClient.java:1698)\n\tat com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClient.invoke(AWSSecurityTokenServiceClient.java:1687)\n\tat com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClient.executeAssumeRole(AWSSecurityTokenServiceClient.java:532)\n\tat com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClient.assumeRole(AWSSecurityTokenServiceClient.java:501)\n\tat com.amazonaws.auth.STSAssumeRoleSessionCredentialsProvider.newSession(STSAssumeRoleSessionCredentialsProvider.java:360)\n\tat com.amazonaws.auth.STSAssumeRoleSessionCredentialsProvider.access$000(STSAssumeRoleSessionCredentialsProvider.java:44)\n\tat com.amazonaws.auth.STSAssumeRoleSessionCredentialsProvider$1.call(STSAssumeRoleSessionCredentialsProvider.java:101)\n\tat com.amazonaws.auth.STSAssumeRoleSessionCredentialsProvider$1.call(STSAssumeRoleSessionCredentialsProvider.java:98)\n\tat com.amazonaws.auth.RefreshableTask.refreshValue(RefreshableTask.java:295)\n\tat com.amazonaws.auth.RefreshableTask.blockingRefresh(RefreshableTask.java:251)\n\tat com.amazonaws.auth.RefreshableTask.getValue(RefreshableTask.java:192)\n\tat com.amazonaws.auth.STSAssumeRoleSessionCredentialsProvider.getCredentials(STSAssumeRoleSessionCredentialsProvider.java:329)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:468)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:431)\n\tat com.wherobots.fs.s3a.WherobotsStIntCredentialsProvider.getCredentials(WherobotsStIntCredentialsProvider.java:184)\n\tat com.wherobots.fs.s3a.WherobotsStIntCredentialsProvider.<init>(WherobotsStIntCredentialsProvider.java:163)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.createAWSCredentialProvider(S3AUtils.java:706)\n\t... 33 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m geojson_multi_path = \u001b[33m\"\u001b[39m\u001b[33ms3://wherobots-examples/data/noaa/storms/\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      3\u001b[39m df = \u001b[43msedona\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgeojson\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmultiLine\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrue\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeojson_multi_path\u001b[49m\u001b[43m)\u001b[49m\\\n\u001b[32m      6\u001b[39m         .selectExpr(\u001b[33m\"\u001b[39m\u001b[33mexplode(features) as features\u001b[39m\u001b[33m\"\u001b[39m)\\\n\u001b[32m      7\u001b[39m         .select(\u001b[33m\"\u001b[39m\u001b[33mfeatures.*\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m df.show(\u001b[32m5\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/spark/python/pyspark/sql/readwriter.py:307\u001b[39m, in \u001b[36mDataFrameReader.load\u001b[39m\u001b[34m(self, path, format, schema, **options)\u001b[39m\n\u001b[32m    305\u001b[39m \u001b[38;5;28mself\u001b[39m.options(**options)\n\u001b[32m    306\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jreader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    308\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    309\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) != \u001b[38;5;28mlist\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/wherobots/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdeco\u001b[39m(*a: Any, **kw: Any) -> Any:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m         converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/wherobots/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    324\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o269.load.\n: java.nio.file.AccessDeniedException: : Instantiate com.wherobots.fs.s3a.WherobotsStIntCredentialsProvider: com.amazonaws.services.securitytoken.model.AWSSecurityTokenServiceException: User: arn:aws:sts::329898491045:assumed-role/wherobots-user-principals/wbc-jf3gkm4ile-besg0oop07pktb is not authorized to perform: sts:AssumeRole on resource: arn:aws:iam::306385193173:role/wherobots-examples-storage-integration-role (Service: AWSSecurityTokenService; Status Code: 403; Error Code: AccessDenied; Request ID: 7fb17b56-88ea-4046-a6ed-4a4c5c382e37; Proxy: null):AccessDenied\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:253)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.createAWSCredentialProvider(S3AUtils.java:745)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.buildAWSProviderList(S3AUtils.java:659)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.createAWSCredentialProviderSet(S3AUtils.java:585)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.bindAWSClient(S3AFileSystem.java:959)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:586)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3611)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3712)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3663)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:724)\n\tat scala.collection.immutable.List.map(List.scala:246)\n\tat scala.collection.immutable.List.map(List.scala:79)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:722)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:551)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:404)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:186)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: com.amazonaws.services.securitytoken.model.AWSSecurityTokenServiceException: User: arn:aws:sts::329898491045:assumed-role/wherobots-user-principals/wbc-jf3gkm4ile-besg0oop07pktb is not authorized to perform: sts:AssumeRole on resource: arn:aws:iam::306385193173:role/wherobots-examples-storage-integration-role (Service: AWSSecurityTokenService; Status Code: 403; Error Code: AccessDenied; Request ID: 7fb17b56-88ea-4046-a6ed-4a4c5c382e37; Proxy: null)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1880)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1418)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1387)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1157)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)\n\tat com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClient.doInvoke(AWSSecurityTokenServiceClient.java:1731)\n\tat com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClient.invoke(AWSSecurityTokenServiceClient.java:1698)\n\tat com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClient.invoke(AWSSecurityTokenServiceClient.java:1687)\n\tat com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClient.executeAssumeRole(AWSSecurityTokenServiceClient.java:532)\n\tat com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClient.assumeRole(AWSSecurityTokenServiceClient.java:501)\n\tat com.amazonaws.auth.STSAssumeRoleSessionCredentialsProvider.newSession(STSAssumeRoleSessionCredentialsProvider.java:360)\n\tat com.amazonaws.auth.STSAssumeRoleSessionCredentialsProvider.access$000(STSAssumeRoleSessionCredentialsProvider.java:44)\n\tat com.amazonaws.auth.STSAssumeRoleSessionCredentialsProvider$1.call(STSAssumeRoleSessionCredentialsProvider.java:101)\n\tat com.amazonaws.auth.STSAssumeRoleSessionCredentialsProvider$1.call(STSAssumeRoleSessionCredentialsProvider.java:98)\n\tat com.amazonaws.auth.RefreshableTask.refreshValue(RefreshableTask.java:295)\n\tat com.amazonaws.auth.RefreshableTask.blockingRefresh(RefreshableTask.java:251)\n\tat com.amazonaws.auth.RefreshableTask.getValue(RefreshableTask.java:192)\n\tat com.amazonaws.auth.STSAssumeRoleSessionCredentialsProvider.getCredentials(STSAssumeRoleSessionCredentialsProvider.java:329)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:468)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:431)\n\tat com.wherobots.fs.s3a.WherobotsStIntCredentialsProvider.getCredentials(WherobotsStIntCredentialsProvider.java:184)\n\tat com.wherobots.fs.s3a.WherobotsStIntCredentialsProvider.<init>(WherobotsStIntCredentialsProvider.java:163)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.createAWSCredentialProvider(S3AUtils.java:706)\n\t... 33 more\n"
     ]
    }
   ],
   "source": [
    "geojson_multi_path = \"s3://wherobots-examples/data/noaa/storms/\"\n",
    "\n",
    "df = sedona.read.format(\"geojson\")\\\n",
    "        .option(\"multiLine\", \"true\")\\\n",
    "        .load(geojson_multi_path)\\\n",
    "        .selectExpr(\"explode(features) as features\")\\\n",
    "        .select(\"features.*\")\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8613e6",
   "metadata": {},
   "source": [
    "### Shapefile\n",
    "\n",
    "The Shapefile format has been around since the early days of GIS and is still used widely in desktop and government datasets.\n",
    "\n",
    "Wherobots can load Shapefiles directly into a Sedona DataFrame using the shapefile data source.\n",
    "This works whether you point to a single .shp file or to a directory containing multiple shapefiles.\n",
    "\n",
    "Wherobots automatically reads the related files (.dbf, .shx, etc.) and converts the geometry column into a native Geometry type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd16fedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "shp_path = \"s3://wherobots-examples/data/austin_boundaries/\"\n",
    "\n",
    "df = sedona.read.format(\"shapefile\").load(shp_path)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db71f6d",
   "metadata": {},
   "source": [
    "When the input path is a directory, all shapefiles directly inside that directory will be loaded.\n",
    "To include shapefiles in subdirectories, set recursiveFileLookup to true:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8d36bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sedona.read.format(\"shapefile\") \\\n",
    "    .option(\"recursiveFileLookup\", \"true\") \\\n",
    "    .load(\"s3://wherobots-examples/data/examples/Global_Landslide_Catalog/\")\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09dcec5",
   "metadata": {},
   "source": [
    "## Loading raster data into a Sedona DataFrame\n",
    "\n",
    "Wherobots can load rasters as native raster columns, allowing you to tile, clip, resample, and compute statistics directly in Spark - just like you would with tabular data.\n",
    "\n",
    "We’ll cover:\n",
    "- Reading GeoTIFFs (COGs recommended)\n",
    "- Reading from STAC APIs and collections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fac4d2",
   "metadata": {},
   "source": [
    "### GeoTIFFs\n",
    "\n",
    "GeoTIFFs are raster image files that store both pixel values and geospatial metadata (like coordinate reference systems).\n",
    "A Cloud-Optimized GeoTIFF (COG) is a GeoTIFF structured for fast, partial reads in cloud storage — ideal for distributed systems like Wherobots.\n",
    "\n",
    "Wherobots' raster reader can load these directly into a Sedona DataFrame.\n",
    "Each raster becomes one or more tiles, stored in a column of type raster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053b2d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "geotiff_path = \"s3://wherobots-examples/data/ghs_population/GHS_POP_E1975_GLOBE_R2023A_4326_3ss_V1_0.tif\"\n",
    "\n",
    "df = sedona.read.format(\"raster\").load(geotiff_path)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cb060b",
   "metadata": {},
   "source": [
    "Each row represents a raster tile, and by default the raster reader automatically:\n",
    "- Splits large rasters into tiles.\n",
    "- Adds x and y columns to indicate each tile’s position.\n",
    "- Reads file-level metadata (CRS, extent, etc.) internally.\n",
    "\n",
    "To fine-tune the tiling behavior, you can specify options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5579d419",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sedona.read.format(\"raster\") \\\n",
    "    .option(\"tileWidth\", \"512\") \\\n",
    "    .option(\"tileHeight\", \"512\") \\\n",
    "    .option(\"retile\", \"true\") \\\n",
    "    .load(geotiff_path)\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef47856",
   "metadata": {},
   "source": [
    "> Tip - Use Cloud-Optimized GeoTIFFs (COGs) when possible — they’re optimized for partial reads in cloud storage, which makes distributed processing far more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e67be62",
   "metadata": {},
   "source": [
    "### SpatioTemporal Asset Catalog (STAC)\n",
    "\n",
    "A SpatioTemporal Asset Catalog (STAC) is a standard for describing and organizing geospatial assets — such as satellite imagery, aerial photos, and elevation data — across space and time.\n",
    "\n",
    "Wherobots includes a built-in STAC Reader, which allows you to load STAC items and collections directly into a Sedona DataFrame. This gives you seamless access to large imagery archives hosted on cloud platforms like AWS, Planetary Computer, or Element84 — all without leaving your Spark environment.\n",
    "\n",
    "The STAC Reader supports:\n",
    "- Direct integration with HTTP, HTTPS, S3, or local STAC JSON sources.\n",
    "- Unified geospatial analysis, so you can join imagery metadata with vector or raster datasets inside Spark.\n",
    "- Spatial and temporal filter pushdown, meaning filters like `ST_Intersects` or datetime BETWEEN are pushed down to the STAC API, minimizing data transfer and improving query performance.\n",
    "- Flexible configuration options for partitioning, request limits, and parallel loading — making it scalable for both exploration and production workflows.\n",
    "\n",
    "You can connect to STAC sources via an HTTPS endpoint, an S3 path, or a local JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72710a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stac_df = sedona.read.format(\"stac\").load(\n",
    "    \"https://earth-search.aws.element84.com/v1/collections/sentinel-2-pre-c1-l2a\"\n",
    ")\n",
    "\n",
    "stac_df.printSchema()\n",
    "stac_df.select(\"id\", \"datetime\", \"geometry\", \"collection\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2620aa",
   "metadata": {},
   "source": [
    "You can control how many items to load, how requests are batched, and how partitions are distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2618b0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sedona.read.format(\"stac\") \\\n",
    "            .option(\"itemsLimitMax\", \"1000\")\\\n",
    "            .option(\"itemsLimitPerRequest\", \"50\")\\\n",
    "            .option(\"itemsLoadProcessReportThreshold\", \"500000\")\\\n",
    "            .load(\"https://earth-search.aws.element84.com/v1/collections/sentinel-2-pre-c1-l2a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bf07e9",
   "metadata": {},
   "source": [
    "## Introduction to Managing Spatial Tables with Iceberg\n",
    "\n",
    "Wherobots extends Apache Iceberg — a modern open table format — to natively support spatial data.\n",
    "This allows you to manage vector and raster datasets just like any other analytical table, with the same reliability, scalability, and query optimization benefits.\n",
    "\n",
    "For a data engineer, this means you can use familiar tools (SQL, Spark, Sedona) while gaining spatial awareness at the table level — no need for custom file handling or geospatial indexing setups.\n",
    "\n",
    "### Why Iceberg Matters for Spatial Data\n",
    "\n",
    "Iceberg brings all the essentials of modern data lake management — schema evolution, ACID transactions, partition pruning, and time travel.\n",
    "Wherobots builds on this foundation to add spatial intelligence directly into the table layer.\n",
    "\n",
    "1. Native Spatial Columns\n",
    "\n",
    "    Geometry and raster columns are first-class types — not just blobs or strings.\n",
    "\n",
    "    This means:\n",
    "    - You can save Sedona DataFrames with geometry or raster columns directly to Iceberg tables.\n",
    "    - Query them with familiar functions like `ST_Intersects`, `ST_Within`, or `RS_Clip`.\n",
    "    - Work with both in-database rasters (stored in memory) and out-of-database rasters (linked to GeoTIFFs or COGs on S3).\n",
    "\n",
    "2. Spatial Statistics and Pruning\n",
    "\n",
    "    Each Iceberg data file stores spatial metadata — including minimum bounding rectangles (MBRs) for geometry and raster columns.\n",
    "\n",
    "    This allows the query engine to:\n",
    "    - Skip irrelevant files that fall outside your spatial filter (spatial pruning).\n",
    "    - Push down bounding-box filters to the scan layer, reducing data read from storage.\n",
    "    - Speed up spatial joins by comparing file-level extents before loading data.\n",
    "\n",
    "3. Spatial Partitioning and Organization\n",
    "\n",
    "    Spatial transformations (like tiling, grid partitioning, or bounding-box bucketing) can be used to organize data.\n",
    "    This helps co-locate nearby geometries and tiles, reducing shuffle and improving performance in spatial joins or aggregations.\n",
    "\n",
    "4. Query Optimization and Pushdown\n",
    "\n",
    "    WherobotsDB uses Iceberg’s metadata to push down both spatial and temporal filters:\n",
    "    - Spatial filters (`ST_Intersects`, `ST_Within`) are evaluated at the metadata level.\n",
    "    - Raster metadata and specific bands can be selectively read (projection pushdown)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010f993a",
   "metadata": {},
   "source": [
    "# Wherobots Fundamentals - Constructing\n",
    "\n",
    "WherobotsDB provides a powerful set of functions to construct geometries. You can either create them from scratch using raw coordinate values (literals) or by parsing standard geospatial data formats like WKT and WKB.\n",
    "\n",
    "---\n",
    "\n",
    "### Creating from Coordinates\n",
    "\n",
    "These functions build geometries directly from numerical inputs.\n",
    "\n",
    "* `ST_MakePoint(x, y, [z], [m])`: Creates a **Point** geometry from its x and y coordinates. You can also optionally provide a z-coordinate (for elevation) and an m-coordinate (a measure value).\n",
    "\n",
    "* `ST_MakeEnvelope(xmin, ymin, xmax, ymax)`: Creates a rectangular **Polygon** that represents a bounding box, or \"envelope,\" from the coordinates of two opposing corners.\n",
    "\n",
    "* `ST_LineStringFromText(text, delimiter)`: This function builds a LineString from a flat string of comma-separated coordinates, like `'x1, y1, x2, y2, ...'`. This provides a fast way to create line geometries directly from raw text data without needing the formal structure of WKT.\n",
    "\n",
    "* `ST_PolygonFromText(text, delimiter)`: Similarly, the ST_PolygonFromText function creates a Polygon from a flat string of comma-separated coordinates. For a valid polygon, the sequence must form a closed ring by ensuring the last coordinate pair is identical to the first (e.g., `'x1, y1, x2, y2, x3, y3, x1, y1'`).\n",
    "\n",
    "---\n",
    "\n",
    "### Creating from Standard Formats\n",
    "\n",
    "These functions parse common text-based or binary geospatial formats.\n",
    "\n",
    "* `ST_GeomFromWKT(text)`: The primary function for constructing any geometry type from its **W**ell-**K**nown **T**ext (WKT) representation. This is one of the most common ways to ingest geometries.\n",
    "\n",
    "* `ST_GeomFromWKB(binary)`: Creates a geometry from its **W**ell-**K**nown **B**inary (WKB) representation, which is a compact, machine-readable alternative to WKT.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Creating from Other Geometries\n",
    "\n",
    "This function combines existing geometries into a single feature.\n",
    "\n",
    "* `ST_Collect(geometry_array)`: Takes an array of geometries and aggregates them into a single multi-part geometry (e.g., `MultiPoint`, `MultiPolygon`) or a `GeometryCollection`. This is useful for grouping related features together. This is an essential function because direct spatial operations on arrays are often limited, so ST_Collect consolidates the individual geometries into one object that can then be analyzed.\n",
    "\n",
    "---\n",
    "\n",
    "While these are common examples, they are not the only constructor functions available in WherobotsDB. We will now look at examples for `ST_MakePoint`, `ST_LineStringFromText`, `ST_Collect`, and `ST_MakeEnvelope`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97066a60",
   "metadata": {},
   "source": [
    "### ST_MakePoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0658995c",
   "metadata": {},
   "outputs": [],
   "source": [
    "points_df = sedona.sql(\"\"\"\n",
    "\n",
    "SELECT ST_MakePoint(-122.349277, 47.620504) as space_needle, ST_MakePoint(-122.350446, 47.620556) as glass_museum, ST_MakePoint(-122.348258, 47.621494) as pop_culture_museum\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "points_df.show(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362730b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_config_url = \"https://raw.githubusercontent.com/wherobots/geospatial-data-engineering-associate/refs/heads/main/assets/week-1/conf/map_config.json\"\n",
    "\n",
    "with open(map_config_url, 'r') as file:\n",
    "    map_config = json.load(file)\n",
    "\n",
    "map = SedonaKepler.create_map(points_df, \"Tourist spots\", map_config)\n",
    "map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d1c3de",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### ST_LineStringFromText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5cf8b5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "line_df = sedona.sql(\"\"\"\n",
    "\n",
    "SELECT ST_LineStringFromText('-122.349277, 47.620504, -122.350446, 47.620556, -122.348258, 47.621494', ',') as order_to_visit\n",
    "\n",
    "\"\"\")\n",
    "line_df.show(1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a6e679",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### ST_Collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d37bee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PS. this allows us to access the dataframes in a SQL environment\n",
    "points_df.createOrReplaceTempView(\"points\")\n",
    "line_df.createOrReplaceTempView(\"line\")\n",
    "\n",
    "collection_df = sedona.sql(\"\"\"\n",
    "\n",
    "SELECT \n",
    "    ST_Collect(Array(space_needle, glass_museum, pop_culture_museum, order_to_visit))\n",
    "FROM points, line\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "collection_df.show(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ca298d",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_collection = SedonaKepler.create_map(collection_df, \"Things to do\", map_config)\n",
    "map_collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a248f2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### ST_MakeEnvelope()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a27d997",
   "metadata": {},
   "outputs": [],
   "source": [
    "envelope_df = sedona.sql(\"\"\"\n",
    "\n",
    "SELECT ST_MakeEnvelope(-122.352848,47.619674,-122.346539,47.622451) AS tourist_location_bbox\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "envelope_df.show(1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c84716a",
   "metadata": {},
   "source": [
    "# Wherobots Fundamentals - Spatial Predicates\n",
    "\n",
    "Spatial predicates are functions that test the relationship between two geometries, returning `TRUE` or `FALSE`. They form the core of most spatial analysis, allowing you to filter data or create joins based on how geometries interact with each other. Understanding the exact logic of each predicate is key to performing accurate analysis.\n",
    "\n",
    "In this section, we will explore some of the most essential predicate functions in detail.\n",
    "\n",
    "-----\n",
    "\n",
    "## ST_Intersects(A, B)\n",
    "\n",
    "This is the most general-purpose spatial relationship, returning `TRUE` if two geometries **share any space at all**. This includes touching at a single point on their boundaries or overlapping in any way. It's the opposite of `ST_Disjoint`.\n",
    "\n",
    "  * **Use Case:** Finding any parcels that intersect with a specific road.\n",
    "\n",
    "-----\n",
    "\n",
    "## ST_Contains(A, B) and ST_Within(A, B)\n",
    "\n",
    "These two functions are opposites and describe a \"spatially-inside\" relationship.\n",
    "\n",
    "  * `ST_Contains(A, B)` returns `TRUE` if geometry **A** completely encloses geometry **B**. No part of B can be outside of A. Think of a cookie inside a cookie jar; the jar contains the cookie.\n",
    "  * `ST_Within(A, B)` returns `TRUE` if geometry **A** is completely inside geometry **B**. It's the reverse of `ST_Contains`. The cookie is within the jar.\n",
    "\n",
    "A key detail is that the boundaries of the geometries cannot simply touch; at least one point of the inner geometry's interior must fall inside the outer geometry's interior. For example, a line that lies perfectly on the boundary of a polygon is not *contained* by it.\n",
    "\n",
    "  * **Use Case:** Finding all the schools (`ST_Within`) a specific city district (`ST_Contains`).\n",
    "\n",
    "-----\n",
    "\n",
    "## ST_Overlaps(A, B)\n",
    "\n",
    "This predicate is more specific than `ST_Intersects`. It returns `TRUE` only if two geometries **partially intersect** and are of the **same dimension**. For example, two overlapping polygons will return `TRUE`, but a line crossing a polygon will not. Critically, neither geometry can be completely contained within the other.\n",
    "\n",
    "  * **Use Case:** Finding sales regions that have a partial overlap, which might indicate a territory dispute.\n",
    "\n",
    "-----\n",
    "\n",
    "### ST_DWithin(A, B, distance, [useSpheroid])\n",
    "\n",
    "Instead of just testing a direct spatial relationship, `ST_DWithin` checks for **proximity**. It returns `TRUE` if the two geometries are **within a specified distance** of each other. This is extremely powerful for \"buffer\" style queries and is highly optimized to use spatial indexes, making it much faster than calculating the exact distance for every pair of geometries.\n",
    "\n",
    "#### Distance Calculation: Spheroid vs. Euclidean\n",
    "\n",
    "The optional `useSpheroid` flag is crucial as it controls how the distance is calculated:\n",
    "\n",
    "* **`useSpheroid = true` (Spheroidal Distance):** This method should be used for geographic data (latitude/longitude). It calculates the more accurate \"great-circle\" distance on a curved surface. When this is enabled, the distance unit is always in **meters**, and the calculation is performed between the centroids of the two geometries.\n",
    "\n",
    "* **`useSpheroid = false` (Euclidean Distance):** This is the default behavior. It performs a simpler, \"flat-earth\" distance calculation. The unit of the `distance` parameter in this case is the same as the unit of the data's Coordinate Reference System (CRS). For accurate results, you should first transform your data into a projected CRS appropriate for distance measurements (e.g., a UTM or State Plane system).\n",
    "\n",
    "* **Use Case:** Finding all ATMs within 500 **meters** of a specific address using geographic coordinates (`useSpheroid = true`), or finding all competing stores within 2,500 **feet** of a location using a projected State Plane CRS where the units are in feet (`useSpheroid = false`).\n",
    "\n",
    "-----\n",
    "\n",
    "These are just a few of the many powerful predicate functions available in WherobotsDB. You can find the complete list in the [official documentation](https://docs.wherobots.com/latest/references/wherobotsdb/vector-data/Predicate/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf7f3c7",
   "metadata": {},
   "source": [
    "# Wherobots Fundamentals - Spatial Joins (Range Joins)\n",
    "\n",
    "Now that you understand spatial predicates, you can use them to perform one of the most powerful operations in geospatial analysis: the **spatial join**. While a standard join uses a key like an ID to match rows (`tableA.id = tableB.id`), a spatial join combines data from two tables based on the spatial relationship between their geometries. This is a type of \"range join\" where the condition isn't simple equality but a spatial test, such as `ST_Intersects(A.geom, B.geom)`.\n",
    "\n",
    "The primary goal of a spatial join is enrichment: adding attributes from one spatial dataset to another based on their shared location.\n",
    "\n",
    "---\n",
    "\n",
    "## Why Spatial Joins Matter\n",
    "\n",
    "Spatial joins allow you to get insights from your data **as if location itself were a column you could join on**. They let you combine completely different datasets using their shared space as the common link, which unlocks powerful analytical capabilities.\n",
    "\n",
    "### Contextual Enrichment\n",
    "\n",
    "Imagine you have a table of customer addresses (points) and a separate table of county demographics (polygons). These tables have no common ID column. A spatial join lets you \"enrich\" your customer data by transferring the demographic information from the county polygon that each customer point falls within. You could then analyze customer behavior by county income level, population density, or any other demographic metric.\n",
    "\n",
    "\n",
    "\n",
    "### Answering Complex Questions\n",
    "\n",
    "Ultimately, spatial joins are how you answer real-world questions that involve location. For example:\n",
    "* Which of our stores are located in flood-prone areas?\n",
    "* What is the average property value for parcels within 500 meters of a new transit line?\n",
    "* How many competitors are within a 10-minute drive of each of our locations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea64a23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "places_df = (\n",
    "    sedona.sql(\"\"\"\n",
    "\n",
    "    WITH seattle_downtown AS (\n",
    "        SELECT ST_GeomFromWKT('POLYGON ((-122.360916 47.590189, -122.299461 47.590189, -122.299461 47.641104, -122.360916 47.641104, -122.360916 47.590189))') AS geom\n",
    "    ),\n",
    "\n",
    "    -- This is to leverage spatial predicate pushdown\n",
    "    places AS (\n",
    "      SELECT *\n",
    "      FROM\n",
    "        wherobots_open_data.overture_maps_foundation.places_place places, seattle_downtown\n",
    "      WHERE\n",
    "        ST_Intersects(places.geometry, seattle_downtown.geom)\n",
    "    ),\n",
    "\n",
    "    -- This is to leverage spatial predicate pushdown\n",
    "    buildings AS (\n",
    "      SELECT *\n",
    "      FROM\n",
    "        wherobots_open_data.overture_maps_foundation.buildings_building buildings, seattle_downtown\n",
    "      WHERE\n",
    "        ST_Intersects(buildings.geometry, seattle_downtown.geom)\n",
    "    )\n",
    "    \n",
    "    SELECT\n",
    "      places.names.primary as place_name,\n",
    "      places.categories.primary as place_type,\n",
    "      element_at(places.addresses, 1) as place_address,\n",
    "      ROUND(places.confidence * 100, 2) AS `place_confidence (%)`,\n",
    "      places.geometry as place_geometry,\n",
    "      buildings.geometry as building_geometry\n",
    "    FROM\n",
    "      places\n",
    "    JOIN\n",
    "      buildings\n",
    "    ON\n",
    "      ST_Intersects(places.geometry, buildings.geometry);\n",
    "    \n",
    "    \"\"\")\n",
    "    # We are caching the result, as we will reuse it to visualize the data\n",
    "        .cache()\n",
    ")\n",
    "\n",
    "places_df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ce0db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_config_join_url = \"https://raw.githubusercontent.com/wherobots/geospatial-data-engineering-associate/refs/heads/main/assets/week-1/conf/map_config_join.json\"\n",
    "\n",
    "with open(map_config_url, 'r') as file:\n",
    "    map_config = json.load(file)\n",
    "\n",
    "map_places = SedonaKepler.create_map(points_df, \"Places in buildings\", map_config)\n",
    "map_places"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c14872c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Creating Havasu Tables from Sedona DataFrames\n",
    "\n",
    "Now that you understand the importance of Apache Iceberg, let's see how simple it is to save a Sedona DataFrame as an Iceberg table. Wherobots uses an enhanced version of Iceberg called **Havasu**, which is purpose-built for high-performance geospatial analytics.\n",
    "\n",
    "---\n",
    "\n",
    "## What is Havasu?\n",
    "\n",
    "Standard Apache Iceberg did not natively support geometry data types until its v3 specification. **Havasu** is Wherobots' enhanced implementation of Iceberg that adds first-class support for both **vector** and **raster** data. This allows you to combine all the benefits of the Iceberg format—like atomic transactions and schema evolution—with native, high-performance geospatial data handling.\n",
    "\n",
    "---\n",
    "\n",
    "## Saving a DataFrame to Havasu\n",
    "\n",
    "Saving a Sedona DataFrame as a Havasu table is a straightforward, one-line command. The process is identical whether your DataFrame contains vector geometries, rasters, or no spatial data at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c53d025a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T15:21:08.465425Z",
     "iopub.status.busy": "2025-10-14T15:21:08.465200Z",
     "iopub.status.idle": "2025-10-14T15:21:08.527899Z",
     "shell.execute_reply": "2025-10-14T15:21:08.527361Z",
     "shell.execute_reply.started": "2025-10-14T15:21:08.465410Z"
    },
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new Havasu (Iceberg) database\n",
    "\n",
    "database = 'gde_bronze'\n",
    "\n",
    "sedona.sql(f'CREATE DATABASE IF NOT EXISTS org_catalog.{database}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635b0c32",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "geotiff_path = \"s3://wherobots-examples/data/ghs_population/GHS_POP_E1975_GLOBE_R2023A_4326_3ss_V1_0.tif\"\n",
    "\n",
    "df = sedona.read.format(\"raster\") \\\n",
    "    .option(\"tileWidth\", \"512\") \\\n",
    "    .option(\"tileHeight\", \"512\") \\\n",
    "    .option(\"retile\", \"true\") \\\n",
    "    .load(geotiff_path)\n",
    "\n",
    "df.writeTo(f\"org_catalog.{database}.ghs_population_tiles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f13332e",
   "metadata": {},
   "source": [
    "# Data validity checks\n",
    "\n",
    "Two of the most common issues with geospatial data include managing projections or Coordinate Reference Systems (CRS) and ensuring geometries are valid.\n",
    "\n",
    "- A geometry is invalid if it violates spatial rules like self-intersections, unclosed rings, misaligned holes, or overlapping parts—making it topologically incorrect.\n",
    "- Spatial files generally contain a Coordinate Reference System or CRS that is defined by a Spatial Reference ID or SRID. This tells us how the data is projected from the round spheroid of the earth onto a flat surface.\n",
    "\n",
    "To fix these issues and ensure our data is valid and in the correct format we use two approaches:\n",
    "\n",
    "1. Check the geometries for any invalidities, and if there are attempt to fix them using `ST_IsValid`, `ST_IsValidDetail`, and `ST_MakeValid`\n",
    "2. Remove or log out any geometries that cannot be fixed\n",
    "3. Standardize our geometries in a single CRS, in this case [EPSG:4326](https://epsg.io/4326) which renders in a coordinate reference system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b71f7ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T15:21:10.936951Z",
     "iopub.status.busy": "2025-10-14T15:21:10.936737Z",
     "iopub.status.idle": "2025-10-14T15:21:10.939286Z",
     "shell.execute_reply": "2025-10-14T15:21:10.938875Z",
     "shell.execute_reply.started": "2025-10-14T15:21:10.936934Z"
    },
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "prefix = 's3://wherobots-examples/gdea-course-data/raw-data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958cc513-5569-413a-9ede-3f5c0e586a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_invalid_geometries(df: DataFrame, geom_col: str = \"geom\", reason_col: str = \"why_invalid\") -> int:\n",
    "    df_with_reason = df.withColumn(reason_col, ST_IsValidReason(col(geom_col)))\n",
    "    # cache to avoid recomputation if you inspect reasons later\n",
    "    df_with_reason.cache()\n",
    "    invalid_count = df_with_reason.filter(~ST_IsValid(col(geom_col))).count()\n",
    "    print(f\"✅ Checked geometries — found {invalid_count} invalid geometries.\")\n",
    "    return invalid_count\n",
    "\n",
    "def fix_invalid_geometries(df: DataFrame, invalid_count: int, geom_col: str = \"geom\") -> DataFrame:\n",
    "    if invalid_count > 1:\n",
    "        print(f\"🔧 Attempting to fix {invalid_count} invalid geometries...\")\n",
    "        return df.withColumn(\n",
    "            geom_col,\n",
    "            when(~ST_IsValid(col(geom_col)), ST_MakeValid(col(geom_col))).otherwise(col(geom_col))\n",
    "        )\n",
    "    else:\n",
    "        print(\"⚡ Only one invalid geometry (or none). Skipping automated fix.\")\n",
    "        return df\n",
    "\n",
    "# --- driver program ---\n",
    "def process_geometries(\n",
    "    df: DataFrame,\n",
    "    geom_col: str = \"geom\",\n",
    "    attempt_fix: bool = True,\n",
    "    split_on_fail: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs validity check -> optional repair -> optional split.\n",
    "    Returns either:\n",
    "      - {\"df\": corrected_df}  when all geometries valid after repair (or none invalid)\n",
    "      - {\"valid_df\": ..., \"invalid_df\": ...} when some invalid remain and split_on_fail=True\n",
    "    \"\"\"\n",
    "    # 1) Initial check\n",
    "    invalid_count = check_invalid_geometries(df, geom_col=geom_col)\n",
    "\n",
    "    if invalid_count == 0:\n",
    "        print(\"✅ All geometries are valid.\")\n",
    "        return {\"df\": df}  # nothing to do\n",
    "\n",
    "    # 2) Attempt repair (only changes rows that are invalid per your earlier contract)\n",
    "    if attempt_fix:\n",
    "        df_fixed = fix_invalid_geometries(df, invalid_count, geom_col=geom_col)\n",
    "        remaining_invalid_count = df_fixed.filter(~ST_IsValid(col(geom_col))).count()\n",
    "        print(f\"🔎 After fixing, {remaining_invalid_count} invalid geometries remain.\")\n",
    "    \n",
    "        if remaining_invalid_count == 0:\n",
    "            print(\"✅ All geometries are valid after fixing.\")\n",
    "            return {\"df\": df_fixed}\n",
    "        elif split_on_fail:\n",
    "            print(\"⚠️ Some invalid geometries remain — splitting dataset.\")\n",
    "            valid_df = df_fixed.filter(ST_IsValid(col(geom_col)))\n",
    "            invalid_df = df_fixed.filter(~ST_IsValid(col(geom_col)))\n",
    "            print(f\"✅ Split complete: {valid_df.count()} valid / {invalid_df.count()} invalid.\")\n",
    "            return {\"valid_df\": valid_df, \"invalid_df\": invalid_df}\n",
    "        else:\n",
    "            print(\"⚠️ Some invalid geometries remain, returning best-effort fixed DataFrame.\")\n",
    "            return {\"df\": df_fixed}\n",
    "    \n",
    "    # If no fix attempt, just split if requested\n",
    "    if split_on_fail:\n",
    "        print(\"⚠️ Skipping fix — splitting into valid and invalid.\")\n",
    "        valid_df = df.filter(ST_IsValid(col(geom_col)))\n",
    "        invalid_df = df.filter(~ST_IsValid(col(geom_col)))\n",
    "        print(f\"✅ Split complete: {valid_df.count()} valid / {invalid_df.count()} invalid.\")\n",
    "        return {\"valid_df\": valid_df, \"invalid_df\": invalid_df}\n",
    "    \n",
    "    print(\"⚠️ Invalid geometries found but no fix or split requested. Returning original DataFrame.\")\n",
    "    return {\"df\": df}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34a2ada-50cd-4bc8-aeb9-4ffc5aa5ef0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEMA Flood Hazard Areas\n",
    "fld_hazard_area = sedona.read.format('shapefile').load(f'{prefix}' + '53033C_20250330/S_FLD_HAZ_AR.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0531c3-b464-426a-b6c2-d4cc2243f0bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = process_geometries(fld_hazard_area, geom_col=\"geometry\", attempt_fix=True, split_on_fail=True)\n",
    "\n",
    "if \"df\" in result:\n",
    "    df_final = result[\"df\"]  # all valid (either already valid or successfully repaired)\n",
    "else:\n",
    "    valid_df = result[\"valid_df\"]\n",
    "    invalid_df = result[\"invalid_df\"]\n",
    "    # handle invalids (e.g., export for manual review)(fld_hazard_area, 'geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654d631f-9d25-47ee-be41-b2480ef9018b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fld_hazard_area.writeTo(f\"org_catalog.{database}.fema_flood_zones_bronze\").createOrReplace()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b016f7b1-3efc-40ca-8eb4-502ae60f1d33",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Transforming CRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384c5eeb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "sedona.sql(f'''\n",
    "select st_srid(geometry) as srid from org_catalog.{database}.fema_flood_zones_bronze limit 1\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2e98f7-4155-437d-910f-98a448dc6ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sedona.sql(f'''\n",
    "select \n",
    "st_srid(st_transform(geometry, 'EPSG:4326')) as srid from org_catalog.{database}.fema_flood_zones_bronze limit 1\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becb1556-cd28-4850-9129-03d99375aba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sedona.sql(f'''\n",
    "select \n",
    "st_srid(st_transform(geometry, 'EPSG:4269', 'EPSG:4326')) as srid from org_catalog.{database}.fema_flood_zones_bronze limit 1\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165d277b",
   "metadata": {},
   "source": [
    "# Loading datasets into WherobotsDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c6b264-da6b-4a4e-bc4f-4e1ad86896da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# King County Generalized Land Use Data\n",
    "gen_land_use = sedona.read.format('shapefile').load(f'{prefix}' + 'General_Land_Use_Final_Dataset/General_Land_Use_Final_Dataset.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19728834-c881-41e2-8449-8737f47343cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_land_use.writeTo(f\"org_catalog.{database}.gen_land_use_bronze\").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd61b53-a501-4144-a796-cf3d8859163a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# King County Sherrif Patrol Districts\n",
    "sherrif_districts = sedona.read.format('shapefile').load(f'{prefix}' + 'King_County_Sheriff_Patrol_Districts___patrol_districts_area/King_County_Sheriff_Patrol_Districts___patrol_districts_area.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8efd8f-36e7-40f2-821b-f7bde9b81179",
   "metadata": {},
   "outputs": [],
   "source": [
    "sherrif_districts.writeTo(f\"org_catalog.{database}.sherrif_districts_bronze\").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d75db79-f5e0-43bf-9383-f96f6a2f65ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# King County Offense Reports\n",
    "offense_reports = sedona.read.format('csv').load(f'{prefix}' + 'KCSO_Offense_Reports__2020_to_Present_20250923.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f367d35a-e5e0-4a94-a5ea-4542a2030e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "offense_reports.writeTo(f\"org_catalog.{database}.offense_reports_bronze\").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01f89e6-6075-4a93-9703-4cc040292382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# King County Bike Lanes\n",
    "bike_lanes = sedona.read.format('shapefile').load(f'{prefix}' + 'Metro_Transportation_Network_(TNET)_in_King_County_for_Bicycle_Mode___trans_network_bike_line/Metro_Transportation_Network_(TNET)_in_King_County_for_Bicycle_Mode___trans_network_bike_line.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac773691-ecaa-46f4-8b01-8407b9059015",
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_lanes.writeTo(f\"org_catalog.{database}.bike_lanes_bronze\").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5929ef8b-0ff9-4178-8d12-f529297945de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEMA National Risk Index\n",
    "fema_nri = sedona.read.format('shapefile').load(f'{prefix}' + 'NRI_Shapefile_CensusTracts/NRI_Shapefile_CensusTracts.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e82d32-ce21-42b3-8589-48cc16998186",
   "metadata": {},
   "outputs": [],
   "source": [
    "fema_nri.writeTo(f\"org_catalog.{database}.fema_nri_bronze\").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab34dd50-0527-49b7-a969-fcea5859b708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# King County School Sites\n",
    "school_sites = sedona.read.format('shapefile').load(f'{prefix}' + 'School_Sites_in_King_County___schsite_point/School_Sites_in_King_County___schsite_point.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a913b76-ec1c-435b-b1e6-694d15ca157f",
   "metadata": {},
   "outputs": [],
   "source": [
    "school_sites.writeTo(f\"org_catalog.{database}.school_sites_bronze\").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517cf454-e658-48a1-955f-56092ad3bf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schools Report Card\n",
    "report_card = sedona.read. \\\n",
    "    format('csv'). \\\n",
    "    load(f'{prefix}' + 'Report_Card_Growth_for_2024-25_20250923.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a37eaa3-fb75-433d-86ec-265d3f6273e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_card.writeTo(f\"org_catalog.{database}.report_card_bronze\").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0907b912-285b-409d-9373-53bf49920b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seismic Hazards\n",
    "seismic_hazards = sedona.read. \\\n",
    "    format('shapefile'). \\\n",
    "    load(f'{prefix}' + 'Seismic_Hazards___seism_area/Seismic_Hazards___seism_area.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d2d673-ce35-48bb-82d1-36d8c432a392",
   "metadata": {},
   "outputs": [],
   "source": [
    "seismic_hazards.writeTo(f\"org_catalog.{database}.seismic_hazards_bronze\").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848c1509-27a0-42ec-b8cc-5b6b58c3287e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Census Block Groups\n",
    "block_groups = sedona.read. \\\n",
    "    format('shapefile'). \\\n",
    "    load(f'{prefix}' + 'tl_2024_53_bg/tl_2024_53_bg.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218d1da0-4276-45c0-8838-ec202257210a",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_groups.writeTo(f\"org_catalog.{database}.block_groups_bronze\").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103d5f5f-ddf8-47f4-a5f1-5fb77496bac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Census CSVs\n",
    "median_age = sedona.read. \\\n",
    "    format('csv'). \\\n",
    "    load(f'{prefix}' + 'ACSDT5Y2023.B01002_2025-09-19T105233/ACSDT5Y2023.B01002-Data.csv')\n",
    "\n",
    "median_age.writeTo(f\"org_catalog.{database}.median_age_bronze\").createOrReplace()\n",
    "\n",
    "total_pop = sedona.read. \\\n",
    "    format('csv'). \\\n",
    "    load(f'{prefix}' + 'ACSDT5Y2023.B01003_2025-09-19T105050/ACSDT5Y2023.B01003-Data.csv')\n",
    "\n",
    "total_pop.writeTo(f\"org_catalog.{database}.total_pop_bronze\").createOrReplace()\n",
    "\n",
    "median_income = sedona.read. \\\n",
    "    format('csv'). \\\n",
    "    load(f'{prefix}' + 'ACSDT5Y2023.B19013_2025-09-19T105253/ACSDT5Y2023.B19013-Data.csv')\n",
    "\n",
    "total_pop.writeTo(f\"org_catalog.{database}.median_income_bronze\").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19610058-ff7d-41fe-807d-7132fec745bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tranist Routes\n",
    "transit_routes = sedona.read. \\\n",
    "    format('shapefile'). \\\n",
    "    load(f'{prefix}' + 'Transit_Routes_for_King_County_Metro___transitroute_line/Transit_Routes_for_King_County_Metro___transitroute_line.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668665e0-8ca0-47c1-ad0b-6de4aa338d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "transit_routes.writeTo(f\"org_catalog.{database}.transit_routes_bronze\").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b97e87d-4079-4f92-8e35-af08b6d0df95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transit Stops\n",
    "transit_stops = sedona.read. \\\n",
    "    format('shapefile'). \\\n",
    "    load(f'{prefix}' + 'Transit_Stops_for_King_County_Metro___transitstop_point/Transit_Stops_for_King_County_Metro___transitstop_point.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c05ba8-9750-41b0-a695-a37fa1b83be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "transit_stops.writeTo(f\"org_catalog.{database}.transit_stops_bronze\").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab603b1-706d-403f-9ba7-bc49820bb224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Water Bodies\n",
    "water_bodies = sedona.read. \\\n",
    "    format('shapefile'). \\\n",
    "    load(f'{prefix}' + 'Waterbodies_with_History_and_Jurisdictional_detail___wtrbdy_det_area/Waterbodies_with_History_and_Jurisdictional_detail___wtrbdy_det_area.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96768e9-9c12-4946-ba3e-7d1bb29c271e",
   "metadata": {},
   "outputs": [],
   "source": [
    "water_bodies.writeTo(f\"org_catalog.{database}.water_bodies_bronze\").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f651b2-135e-483a-b376-1722e1b69d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wildfire Polygons\n",
    "wildfires = sedona.read. \\\n",
    "    format('shapefile'). \\\n",
    "    load(f'{prefix}' + 'Wildfires_1878_2019_Polygon_Data/Shapefile/US_Wildfires_1878_2019.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f036a2b1-a97d-4731-ad41-fd73d87296f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wildfires.writeTo(f\"org_catalog.{database}.wildfires_bronze\").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278f504d-8689-4c71-bbb1-bf9d2697dab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elevation\n",
    "\n",
    "url = 's3://copernicus-dem-30m/*/*.tif'\n",
    "elev_df = sedona.read.format(\"raster\").option(\"retile\", \"true\").load(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f7e496-b4fa-41e7-a418-afafc8b818b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "elev_df.writeTo(f\"org_catalog.{database}.elevation_bronze\").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8270605c-65ae-4222-8975-554d17b03b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geocoded Schools\n",
    "schools = sedona.read. \\\n",
    "    format('geojson'). \\\n",
    "    option('mode', 'DROPMALFORMED'). \\\n",
    "    load(f'{prefix}' + 'Washington_State_Public_Schools_GeoCoded.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c889f2-9210-4c03-b6a1-5357929ea913",
   "metadata": {},
   "outputs": [],
   "source": [
    "schools = schools \\\n",
    "    .withColumn(\"geometry\", expr(\"geometry\")) \\\n",
    "    .withColumn(\"AYPCode\", expr(\"properties['AYPCode']\")) \\\n",
    "    .withColumn(\"CongressionalDistrict\", expr(\"properties['CongressionalDistrict']\")) \\\n",
    "    .withColumn(\"County\", expr(\"properties['County']\")) \\\n",
    "    .withColumn(\"ESDCode\", expr(\"properties['ESDCode']\")) \\\n",
    "    .withColumn(\"ESDName\", expr(\"properties['ESDName']\")) \\\n",
    "    .withColumn(\"Email\", expr(\"properties['Email']\")) \\\n",
    "    .withColumn(\"GeoCoded_X\", expr(\"properties['GeoCoded_X']\")) \\\n",
    "    .withColumn(\"GeoCoded_Y\", expr(\"properties['GeoCoded_Y']\")) \\\n",
    "    .withColumn(\"GradeCategory\", expr(\"properties['GradeCategory']\")) \\\n",
    "    .withColumn(\"HighestGrade\", expr(\"properties['HighestGrade']\")) \\\n",
    "    .withColumn(\"LEACode\", expr(\"properties['LEACode']\")) \\\n",
    "    .withColumn(\"LEAName\", expr(\"properties['LEAName']\")) \\\n",
    "    .withColumn(\"LegislativeDistrict\", expr(\"properties['LegislativeDistrict']\")) \\\n",
    "    .withColumn(\"LowestGrade\", expr(\"properties['LowestGrade']\")) \\\n",
    "    .withColumn(\"MailingAddress\", expr(\"properties['MailingAddress']\")) \\\n",
    "    .withColumn(\"NCES_X\", expr(\"properties['NCES_X']\")) \\\n",
    "    .withColumn(\"NCES_Y\", expr(\"properties['NCES_Y']\")) \\\n",
    "    .withColumn(\"Phone\", expr(\"properties['Phone']\")) \\\n",
    "    .withColumn(\"Principal\", expr(\"properties['Principal']\")) \\\n",
    "    .withColumn(\"School\", expr(\"properties['School']\")) \\\n",
    "    .withColumn(\"SchoolCategory\", expr(\"properties['SchoolCategory']\")) \\\n",
    "    .withColumn(\"SchoolCode\", expr(\"properties['SchoolCode']\")) \\\n",
    "    .withColumn(\"SingleAddress\", expr(\"properties['SingleAddress']\")) \\\n",
    "    .drop(\"properties\").drop(\"type\") \\\n",
    "    .drop(\"_corrupt_record\").drop(\"type\") \\\n",
    "    .drop(\"type\").drop(\"type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ce1324-82c6-4300-8389-70e3b4935044",
   "metadata": {},
   "outputs": [],
   "source": [
    "schools.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ac3d21-b73c-475c-b3f0-e8254de09b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "schools.writeTo(f\"org_catalog.{database}.schools_bronze\").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0276f986-4389-4e68-aa50-f08081824a76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
